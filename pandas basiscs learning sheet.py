# PANDAS

import pandas as pd
import numpy as np

#pandas series
"""

#just a table with columns

#create table

python_list = [1,5,7,9]
my_series = pd.Series(python_list)
print(my_series)

# --> 0    1
#     1    5
#     2    7
#     3    9

#pull item out of series

my_item = my_series[1]
print(my_item)

#Labels using the index argument (create names for the columns)

my_indexes = ["A","B","C","D"]
new_series = pd.Series(data= python_list,index=  my_indexes)
print(new_series)

#the index on the left is no our letters:
#   A    1
#   B    5
#   C    7
#   D    9

#pull out a row by its name

print(new_series["B"]) #    --> 5

#create a series by a python dictionary
my_dict = {"Tesla":12, "Mercedes":10, "BMW": 15}
car_series = pd.Series(my_dict)
print(car_series)

#pull out item by name

print(car_series["Tesla"]) #    --> 12
"""

#data frames in pandas
"""

#create a data frame with random numbers using numpy.random

my_data = np.random.randn(4,3) #Rows, Columns
my_rows = ["A","B","C","D"]
my_cols = ["Monday","Tuesday","Friday"]

#now we got rows, columns and content
#lets create the data frame

my_df = pd.DataFrame(data= my_data,index= my_rows,columns= my_cols)
print(my_df)

#We created a dataframe with 4 rows, 3 columns, with the rows being indexed A to D and the columns names Monday, Tuesday and Friday
#As content we used numbers randomly generated by np.random

#most times you want to read a file and put it into table form instead of creating the dataframe manually

#import csv file

my_df2 = pd.read_csv("email.csv")
print(my_df2)

#if the file is installed pandas will automatically create rows, columns and content from the file

#pull out rows

first_row = my_df2.loc[0]

print(first_row)        # --> Login email;Identifier;First name;Last name    laura@example.com;2070;Laura;Grey

#pull out multiple rows

my_rows = my_df2.loc[[0,1,2]]
print(my_rows)

#  Login email;Identifier;First name;Last name
#0           laura@example.com;2070;Laura;Grey
#1        craig@example.com;4081;Craig;Johnson
#2          mary@example.com;9346;Mary;Jenkins
"""

#pull out data from data frames
"""
my_df = pd.read_csv("dog_data.csv")

#grab first five rows

beginning = my_df.head(5)
print(beginning)

#grab last five rows

ending = my_df.tail(5)
print(ending)

#head and tail get the the first/last x rows

#get info about dataframe 

info = my_df.info()
print(info)         # --> all kinds of info stuff

#get shape (rows & columns)

shape = my_df.shape     # not a function
print(shape)            # 2670 rows, 4 columns

#get dimensions

dims = my_df.ndim
print(dims)

#get column datatypes

c_dtype = my_df.dtypes
print(c_dtype)

#get some statistics about the data

stats = my_df.describe()
print(stats)

#get statistics of columns

breed_description = my_df["Breed"].describe()
color_description = my_df["Color"].describe()
print(breed_description, color_description)


#select specific column

my_df["DogName"]
my_df.DogName
my_df.iloc[:,2]

#all the above are the same thing
"""

# Advanced Data Input/Output Operations
"""
# Reading data from an Excel file
# Make sure you have 'openpyxl' installed for Excel file handling
# Install it using: pip install openpyxl

excel_file = 'data.xlsx'  # Replace with your Excel file path
df_from_excel = pd.read_excel(excel_file)
print("Data read from Excel file:")
print(df_from_excel.head())  # Displaying the first few rows

# Writing DataFrame to a JSON file
json_file = 'output.json'  # Specify your JSON file name
df_from_excel.to_json(json_file, orient='records', lines=True)
print(f"Data written to JSON file: {json_file}")

# Reading data from a JSON file
df_from_json = pd.read_json(json_file, orient='records', lines=True)
print("Data read from JSON file:")
print(df_from_json.head())  # Displaying the first few rows

# Writing DataFrame to an Excel file
# Make sure you have 'openpyxl' installed for Excel file handling
output_excel_file = 'output_data.xlsx'  # Specify your output Excel file name
df_from_json.to_excel(output_excel_file, index=False)
print(f"Data written to Excel file: {output_excel_file}")
"""

#count data from dataframe
"""
my_df = pd.read_csv("dog_data.csv")

#how many dogs with a specific color combination are there?

color_combs = my_df["Color"].value_counts()
print(color_combs)

#we get a series of rowname= color, rowdata= number of dogs with that specific color
#for example: BLACK 371, BROWN 309 ...
#these are in DESCENDING ORDER

color_combs_2 = my_df["Color"].value_counts(ascending= True)
print(color_combs_2)

#now its ASCENDING

#sometimes part of the data is missing or not properly noted. These entraces in the csv file will result in a "NaN" = nullvalue

#get the nullvalues as well:

dog_names = my_df["DogName"].value_counts(dropna= False)
print(dog_names)

#get relative frequency (percentage), what percentage of dogs is fE. black?

color_combs = my_df["Color"].value_counts(normalize= True)
print(color_combs)                # 13% black, 11% brown, ...


#get white color SPECIFICALLY

color_white = my_df["Color"].value_counts()["WHITE"]     #PANDAS IS CASE-SENSATIVE!!!!
print(color_white)  # --> 0.08480300187617261, 8percent are white

#count unique value - size

color_size = my_df.groupby("Color").size()
print(color_size)       # alphabetical order

#count unique values - count
color_count = my_df.groupby("Color").count()
print(color_count) 

#get number of all dogs called CHARLEY

charleys = my_df["DogName"].value_counts()["CHARLEY"]
print(charleys)         # --> 5

#5 dogs are named charley

"""

#add and remove columns/rows to dataframes
"""

my_df = pd.read_csv("dog_data_short.csv")

#add column from a regular python list

gender= ["male","female","male","male","female"] 
my_df["Gender"] = gender

print(my_df)

#we added a column of the gender
#this was only possible was number of entrances in the list (5) matches the number of items in the dataframe (also 5)

#but what if we have a dataset of 150000 rows?

#add default values

my_df["Alive/Dead"] = [True] * len(my_df)
print(my_df)

#we added a new column (Alive / Dead) with the DEFAULT VALUE = True, which is a list which automatically takes on the length of the dataframe


#add NaN Values (we dont know the value yet)

my_df["ShowDog"] = [np.nan] * len(my_df)

#add column with .insert()
#.insert(index, name, value, allow_duplicates= True)

my_df.insert(1,"adopted", [True]*len(my_df), True)
print(my_df)

#add column with .assign() --> creates a new dataframe instead of changing one

new_df = my_df.assign(Horse= [False]*len(my_df))
print(new_df)
print(my_df)            # --> not the same

#remove column
#drop(name, axis, delete_forever_or_not)

new_df.drop("Alive/Dead", axis= 1, inplace= True) #1st axis is the column axis
print(new_df)

new_df.drop("Gender", axis= 1, inplace= True) #1st axis is the column axis
print(new_df)

#remove rows
new_df.drop(3, inplace= True)      #axis automatically equals 0 which is the row axis
print(new_df)
"""

#grab row point subsets
"""

my_df = pd.read_csv("dog_data_short.csv")

#grab a row by location

row_two = my_df.loc[2]      # --> 2 is the name of the row
print(row_two)

#grab row by index location

row_three = my_df.iloc[3]      # --> 3 is the index of the row
print(row_three)

#grab specific point , my_df.loc[row, column]
# DogName TACODA

name_row_one = my_df.loc[1, "DogName"]
print(name_row_one)

#SubSets

my_data = my_df.loc[[1,3],["DogName","Breed"]]
print(my_data)

#   DogName        Breed
#1  TACODA  GER SHEPHERD
#3   ARROW         MIXED

my_data = my_df.loc[[1,3,4],["DogName","Breed"]]
print(my_data)
"""

#conditional selections for dataframes
"""

my_df = pd.read_csv("dog_data.csv")

#get a dataframe where all values == "BROWN" are True and all values != "BROWN" are False

boolean = my_df == "BROWN"
print(boolean)

#return dataframe with data

filtered_df = my_df[my_df == "BROWN"]
print(filtered_df)

#return only on one column

filtered_column = my_df[my_df == "BROWN"]["Color"]
print(filtered_column)

#return on multiple columns

filtered_columns = my_df[my_df == "BROWN"][["Color","Breed"]]
print(filtered_columns)

#ofcourse none of our data in "Breed" equals "BROWN"
#much more interesting when comparing 2 dataframes of numbers

#multiple conditions:
#AND statement (&)

two_conditions = my_df[(my_df["Color"]=="BROWN") & (my_df["Breed"] == "MIXED")]
print(two_conditions)

#get length

length = len(my_df[(my_df["Color"]=="BROWN") & (my_df["Breed"] == "MIXED")])
print(length)      # --> 38 entries

#OR statement (|)

two_conditions = my_df[(my_df["Color"]=="BROWN") | (my_df["Breed"] == "MIXED")]
print(two_conditions)

#get length

length = len(my_df[(my_df["Color"]=="BROWN") | (my_df["Breed"] == "MIXED")])
print(length)      # --> 506 entries

#return specific column for all the entries where condition is met

DogNames_cond = my_df[(my_df["Color"]=="BROWN") & (my_df["Breed"] == "MIXED")]["DogName"]
print(DogNames_cond)
"""

# reset/change index
"""
my_df = pd.read_csv("dog_data_short.csv")

#create a new column

my_df["Frame Header"] = ["Dog 1", "Dog 2", "Dog 3", "Dog 4", "Dog 5"]
print(my_df)

# set index

my_df.set_index("Frame Header")
print(my_df)        # --> nothing changed

my_df.set_index("Frame Header", inplace= True)
print(my_df)        # --> changed

#reset index

my_df.reset_index(inplace= True)
print(my_df)        # Frame Header is a column again

#delete column

my_df.drop("Frame Header",axis= 1, inplace= True)   # --> back to original
"""

#fix incomplete data
"""

dummy_data= {"A":[1,2,3],"B":[4,np.nan,6],"C":[7,8,9],"D":[10,11,12]}
my_df = pd.DataFrame(dummy_data)
print(my_df)

#"B"[1] is missing  --> incomplete data

#drop the rows with nulldata

print(my_df.dropna())

#get rid of columns with null data

print(my_df.dropna(axis= 1))

#new df with more null values
dummy_data= {"A":[1,2,3],"B":[4,np.nan,6],"C":[np.nan,np.nan,9],"D":[10,11,12]}
my_df = pd.DataFrame(dummy_data)
print(my_df)

#get only rid of columns with at least 2 nullvalues

print(my_df.dropna(thresh= 2, axis= 1))    # --> somehow this only works with columns

#replace data

dummy_data= {"A":[1,2,3],"B":[4,np.nan,6],"C":[7,8,9],"D":[10,11,12]}
my_df = pd.DataFrame(dummy_data)

print(my_df.fillna(value= True))

#use math functions

my_df.fillna(value=my_df["B"].mean(), inplace= True)
print(my_df)
"""

# group by function
"""

data= {
    "Corporation": ["Apple","Google","Meta","Apple","Google","Meta"],
    "Employees":   ["John","April","Wes","Beth","Justin","Steph"],
    "Salary":      [200,220,190,130,120,150]
}
my_df = pd.DataFrame(data)

print(my_df)

#nothing is sorted or grouped together, the dataframe is messy and all over the place
#group by corporation

company = my_df.groupby("Corporation")

#sum

print(company.sum())

#mean

print(company.mean())

#min / max

print(company.min())
print(company.max())

#standard deviation , variance

print(company.std())
print(company.var())

#count

print(company.count())

#groups the data by one column
"""

#counting unique data
"""

my_df = pd.read_csv("dog_data.csv")

#count values

DogNames = my_df["DogName"].value_counts()
print(DogNames)

#grab unqiue values

unique_DogNames = my_df["DogName"].unique()
print(unique_DogNames)
"""

#apply function on dataframes
"""

data= {
    "Corporation": ["Apple","Google","Meta","Apple","Google","Meta"],
    "Employees":   ["John","April","Wes","Beth","Justin","Steph"],
    "Salary":      [200,220,190,130,120,150]
}
my_df = pd.DataFrame(data)

#create a python function

def times1000(x):
    return x * 1000

new_df = my_df["Salary"].apply(times1000)
print(new_df)       # all salaray values got multiplied by 1000

# add elder last name

def namer(x):
    if x == "John":
        return "John Elder"
    else:
        return x
    
new_df = pd.DataFrame({
    "Employees": my_df["Employees"].apply(namer),
    "Salary": my_df["Salary"].apply(times1000)
})
print(new_df)

#use lambda 
new_df = my_df["Salary"].apply(lambda x: format( x* 1000,","))
print(new_df)

#append to current dataframe 

my_df["Salarytimes1000"] = my_df["Salary"].apply(times1000)
print(my_df)

#apply on multiple columns

def namer(x):
    return "ARR, " + x

my_df [["Corporation","Employees"]] = my_df[["Corporation","Employees"]].apply(namer)
print(my_df)
"""

#sorting and ordering data 
"""

data= {
    "Corporation": ["Apple","Google","Meta","Apple","Google","Meta"],
    "Employees":   ["John","April","Wes","Beth","Justin","Steph"],
    "Salary":      [200,220,190,130,120,150]
}
my_df = pd.DataFrame(data)

#salary column is not ordered AT ALL

my_df.sort_values("Salary", inplace= True)
print(my_df)            # --> ASCENDING ORDER

my_df.sort_values("Salary", inplace= True, ascending= False)
print(my_df)            # --> DESCENDING ORDER

# sort strings alphabetically

my_df.sort_values("Corporation", inplace= True)
print(my_df)  
"""


#histograms with pandas and matplotlib
"""

import matplotlib.pyplot as plt

my_df = pd.DataFrame(np.random.randn(100, 4),columns= ["Mon","Tues","Wed","Thur"])
print(my_df)

#so far we only used the print function to visualize the data. This is ofcourse not very practical for any dataframe with more than 10 entries

# Histogram

Histogram = my_df["Wed"].hist()
#plt.show()

#Histogram with smaller bins

my_df["Wed"].hist(bins= 50)
#plt.show()

#another way

plt.figure()
my_df["Wed"].plot(kind= "hist", bins= 100)
#plt.show()

#add legend

plt.close("all")
plt.figure()
my_df["Wed"].plot(kind= "hist", bins= 100, grid= False, legend= True)
plt.show()
"""

#area plots with pandas and matplotlib
"""

import matplotlib.pyplot as plt

my_df = pd.DataFrame(np.random.randn(100, 4),columns= ["Mon","Tues","Wed","Thur"])
print(my_df)

#.plot(kind= "area") can usually only show positive data, our data is mixed though which is why we can either

#modify our data:

my_df.abs().plot(kind= "area")
#plt.show()

#or modify the fucntion to show negative values also

plt.close()
my_df.plot(kind= "area", stacked= False)
#plt.show()

#one of them shows only positive, the other also negative values

#add shading

plt.close()
my_df.abs().plot(kind= "area", alpha= 0.3)
#plt.show()

#add title

plt.close()
my_df.abs().plot(kind= "area", alpha= 0.3, title= "Area plot")
#plt.show()


#remove legend

plt.close()
my_df.abs().plot(kind= "area", alpha= 0.3, title= "Area plot", legend= False)
#plt.show()

#plot just one

my_df["Mon"].abs().plot(kind= "area", alpha= 0.3, title= "Area plot", legend= False)

#plot two

plt.close()
my_df[["Mon","Thur"]].abs().plot(kind= "area", alpha= 0.3, title= "Area plot", legend= False)

#plt.show()

#add table

plt.close()
my_df.abs().plot(kind= "area", alpha= 0.3, title= "Area plot", table= True)
plt.show()      # --> looks scuffed, no data in there
"""

#bar charts with pandas and matplotlib
"""

import matplotlib.pyplot as plt

my_df = pd.DataFrame(np.random.randn(30, 4),columns= ["Mon","Tues","Wed","Thur"])

#bar plot

my_bargraph = my_df.abs().plot(kind="bar",)
#plt.show()

#stack the bars
plt.close()
my_bargraph = my_df.abs().plot(kind="bar", stacked =True)
#plt.show()

#shading
plt.close()
my_bargraph = my_df.abs().plot(kind="bar", stacked =True, alpha= 0.3)
#plt.show()

#second way
my_bargraph = my_df.abs().plot.bar  # same thing
plt.close()
"""

#line charts with pandas and matplotlib
"""
import matplotlib.pyplot as plt

my_df = pd.DataFrame(np.random.randn(30, 4),columns= ["Mon","Tues","Wed","Thur"])

line_graph = my_df.plot(kind="line")
#plt.show()

#show only one column

plt.close()
line_graph = my_df["Wed"].plot(kind="line")
#plt.show()

#line width

plt.close()
line_graph = my_df["Wed"].plot(kind="line", lw=5)
#plt.show()

# size and title

plt.close()
line_graph = my_df["Wed"].plot(kind="line", lw=5, figsize=(10,5), title= "Wednesday linechart")
#plt.show()

#second way

plt.close()
my_df.plot.line()
plt.show()
""" 

#scatterplots with pandas and matplotlib
"""

import matplotlib.pyplot as plt

my_df = pd.DataFrame(np.random.randn(300, 4),columns= ["Mon","Tues","Wed","Thur"])

#create a scatter plot
#scatter needs x and y coordinates

scatter_plot= my_df.plot(kind="scatter", x="Mon", y="Tues")
#plt.show()

#offset against wednesday

plt.close()
scatter_plot= my_df.plot(kind="scatter", x="Mon", y="Tues", c="Wed")
#plt.show()

#change colorscheme with colormap
#"Greys" --> black and white, "Blues" --> white to dark blue, etc...


plt.close()
scatter_plot= my_df.plot(kind="scatter", x="Mon", y="Tues", c="Wed", colormap= "magma")
#plt.show()

#change size

plt.close()
scatter_plot= my_df.plot(kind="scatter", x="Mon", y="Tues", s="Wed", colormap= "magma")
#plt.show()         # --> now its WAY too small

#better way to do it

plt.close()
scatter_plot= my_df.plot(kind="scatter", x="Mon", y="Tues", s=my_df["Wed"]*30)
plt.show()
""" 

#boxplots with pandas and matplotlib
"""

import matplotlib.pyplot as plt

my_df = pd.DataFrame(np.random.randn(300, 4),columns= ["Mon","Tues","Wed","Thur"])

boxplot = my_df.plot(kind="box")
#plt.show()

#boxplots size

plt.close()
boxplot = my_df.plot(kind="box", figsize= (10,5))
#plt.show()

#boxplot color
#   --> full list of colors on the internet

plt.close()
boxplot = my_df.plot(kind="box", figsize= (10,5), cmap="Greys")
#plt.show()

#NO LEGENDS POSSIBLE FOR BOXPLOTS

#other way

plt.close()
boxplot = my_df.plot.box()
#plt.show()
""" 

#hexbinplots with pandas and matplotlib
"""

import matplotlib.pyplot as plt

my_df = pd.DataFrame(np.random.randn(300, 4),columns= ["Mon","Tues","Wed","Thur"])

#create a hexbinplot
#requires x and y coordinates

hoxbinplot = my_df.plot(kind="hexbin", y="Tues", x= "Mon")
#plt.show()         # again WAY too small

#the bigger the gridsize number the more it zooms out

plt.close()
hoxbinplot = my_df.plot(kind="hexbin", y="Tues", x= "Mon", gridsize= 10)
#plt.show()          # now it looks very freakin cool

#hexplots color
# --> all colors can be found on the matplotlib website

plt.close()
hoxbinplot = my_df.plot(kind="hexbin", y="Tues", x= "Mon", gridsize= 10, cmap="Greys")
#plt.show()

#other method

plt.close()
hoxbinplot = my_df.plot.hexbin(y="Tues", x= "Mon", gridsize= 10)
#plt.show()
""" 

#density and KDE plots with pandas and matplotlib
"""

import matplotlib.pyplot as plt
#NEED SCIPY FOR THIS

my_df = pd.DataFrame(np.random.randn(300, 4),columns= ["Mon","Tues","Wed","Thur"])

#create a density plot

density = my_df.plot(kind="kde")
#plt.show()

density = my_df.plot(kind="density")
#plt.show()

#the same thing
plt.close("all")

#Density plot

my_df.plot.density()

#a single line

plt.close()
density = my_df["Wed"].plot(kind="density")
#plt.show()

#change linewidth

plt.close()
density = my_df["Wed"].plot(kind="density", lw= 10)
#plt.show()
"""